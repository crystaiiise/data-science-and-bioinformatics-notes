{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc08445-9b69-4eac-a65c-6e2022ecd1cc",
   "metadata": {},
   "source": [
    "**Ch10: 爬虫基础**\n",
    "\n",
    "Source: \n",
    "[数据分析实战 45 讲](https://time.geekbang.org/column/intro/100021701?tab=intro)\n",
    "\n",
    "---\n",
    "\n",
    "## 爬虫前的背景知识\n",
    "\n",
    "三月份记的，真是太小白了 -_-）\n",
    "\n",
    "### HTTP (HyperText Transfer Protocol)\n",
    "\n",
    "用于在网络上传输超文本(如HTML）的protocol，是客户端（如浏览器）和服务器之间通信的基础。Python上解析它用到request库，分`get()`, `post()`，后者用于向服务器提交数据，比如文件、表单。\n",
    "\n",
    "`request.get()`：\n",
    "\n",
    "- status_code：200: successfull; 404: not found; 403: forbidden: 500: server error\n",
    "\n",
    "- headers: 伪装这个爬虫是个浏览器\n",
    "\n",
    "    - content-type: 用于指示返回内容的类型。\n",
    " \n",
    "    - `print(response.headers['Content-Type'])`此时为API接口，返回json数据。直接供程序使用，无需渲染页面。 \n",
    "               \n",
    "    - 常见content-type有：text/html：HTML页面；application/json：JSON数据；image/png。\n",
    "\n",
    "### 正在使用markdown，那么HTML和XML这些markup标记语言是什么？？\n",
    "\n",
    "- 标记语言是一种用成对标签（`<tag>`和`</tag>`，也就是HTML中说的**node/节点**）描述文档结构和内容的语言。\n",
    "\n",
    "- 而Markdown是一种轻量级语言，用简单的符号（如 `#`、`*`）格式化文本。\n",
    "\n",
    "### HTML (HyperText Markup Langugae) \n",
    "\n",
    "一种用于创建网页的语言啦。网页的骨架，有一系列标签。\n",
    "\n",
    "**即HTTP负责将HTML文件从服务器传输到客户端（如浏览器）**；HTML是内容格式，定义了网页的结构和内容，浏览器解析HTML并渲染成用户看到的页面。\n",
    "\n",
    "XPath是用于在HTML文档中提取元素的选择器，又有它自己的一套语言。而BeautifulSoup提供类似Python的链式方法（eg. `find()`, `select()`），不像XPath那样有自己的表达式，也可以用Regex。\n",
    "\n",
    "---\n",
    "\n",
    "##  爬虫流程\n",
    "\n",
    "### 打开网页\n",
    "\n",
    "用requests库发送HTTP请求。也可以Selenium用于自动化浏览器操作（如处理动态加载的页面），但这个开销大，一般先用requests快速获取数据，若遇到动态内容（如JavaScript渲染的那种网页，那种打开源码也找不着的必须Cmd+Shift+C 'Inspect element'的），再改用Selenium。但是Selenium也可以自动化点击等等！很好玩喵，后面某章会讲到。\n",
    "\n",
    "### 提取并保存数据\n",
    "\n",
    "- 解析器，比如BeautifulSoup方法中的'html.parser'参数及lxml可将request打开之后的HTML文档解析成Python对象\n",
    "\n",
    "- 选择器，在HTML或XML这样子的文档里定位并提取想找的元素，比如HTML需要用XPath或者CSS选择器。用BeautifulSoup提取元素的话可以用Regex进行**字符串匹配**，需导入re库。如果是json数据，用json库解析成Python对象：`json.load()`；`json.dumps()`转换回来。\n",
    "\n",
    "- 保存至csv/xls。\n",
    "\n",
    "---\n",
    "\n",
    "## XPath Syntax\n",
    "\n",
    "```\n",
    "//tagname[@attribute = ‘value’]\n",
    "```\n",
    "\n",
    "注意！！它**不像regex一样是匹配夹在两边之间的**，而是以**节点内部的**（从前一个`<`到相匹配的`>`结束）选择，除非`text()`。\n",
    "\n",
    "* /：从根节点开始（绝对路径）。\n",
    "  \n",
    "* //：从**任意**位置匹配（相对路径）。相对路径想要增加父节点作为约束也是用俩斜杠，eg. `//div[@class=\"title\"]//a[@class=\"title-text\"]`\n",
    "\n",
    "* .：当前节点; ..：父节点。\n",
    "\n",
    "* @：选择attribute，`<tagname attribute=\"value\">`。\n",
    "\n",
    "    * `//div[contains(@id, 'something')]` 取`<div id=\"...something...\">`的节点。\n",
    "\n",
    "    * `//div[starts-with(@id, 'something')]` 取`<div id=\"something...\">`的节点。\n",
    " \n",
    "* \\*：匹配任意(全部）节点。\n",
    "\n",
    "* \\@*：匹配任意(全部）属性。\n",
    "\n",
    "* []：**用于过滤节点**（不然怎么知道相对路径匹配到的是哪一个）\n",
    "\n",
    "    * `//div[@*]`选择所有带有属性的div\n",
    "    \n",
    "    * `//div[1]`选择第一个div\n",
    "    \n",
    "    * `//div[last()]`选择最后一个div\n",
    "    \n",
    "    * `//div[position()<3]`选择前两个div\n",
    "    \n",
    "    * `//div[@class=\"box\"]`选择class属性为\"box\"的div\n",
    "    \n",
    "    * `//p/text()`提取所有成对`<p>`之间的文本。\n",
    "\n",
    "#### 重点表格！！\n",
    "\n",
    "| XPath expression | What it matches | Example output (for `<a href=\"/song?id=123\">Link</a>`) |\n",
    "|------------------|-----------------|--------------------------------------------------------|\n",
    "| `//a`            | All `<a>` element nodes in the document | `<a href=\"/song?id=123\">Link</a>` |\n",
    "| `//a/@href`      | All `href` attribute **values** of `<a>` elements | `\"/song?id=123\"` (*just the URL string*) |\n",
    "| `//a/text()`     | All **text content** inside `<a>` elements | `\"Link\"` |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Regex 及其在HTML中用法\n",
    "\n",
    "- Scrutinize the actual source code.\n",
    "\n",
    "- Use raw strings (`r'pattern'`)，要不然有的python字符还得转义，加上这个就只用转义regex的特殊字符（下面有列举）。\n",
    "\n",
    "### Basic re.functions()\n",
    "\n",
    "- `re.findall()` - Returns all matches as a list;\n",
    "\n",
    "- `re.search()` - Returns a match object for the first match;\n",
    "\n",
    "- `re.match()` - Checks only at string beginning;\n",
    "\n",
    "- `re.sub()` - Replaces matches.\n",
    "\n",
    "### Common patterns:\n",
    "\n",
    "* `.*` - Matches any character *(except newlines eg. `\\n`/`\\r`, but includes whitespaces)* 0+ times. `+` - 1+ times; `?` - 0/1 times (optional); {n} - exactly n times (eg. `\\d{3}` -> \"123\"); {n,} - n or more times.\n",
    "\n",
    "    * `.*?` - Non-greedy version (matches **as little as possible**)\n",
    "\n",
    "* `\\d` - Digit, **`\\s` - whitespace** (space, tab, newline), `\\w` - word character (a-z, A-Z, 0-9, _).\n",
    "\n",
    "* `\\D` - Non-digit, `\\S` - non-whitespace, `\\W` - non-word character (eg. '@').\n",
    "\n",
    "* `^` - Start of string: `^\\d` -> \"1 apple\"; `$` - end of string: `\\d$` -> \"apple 2\".\n",
    "\n",
    "* `\\[^ ]` Any character **not** in brackets: `[^\\d]` -> \"a\", \"-\"\n",
    "\n",
    "* `( )` Captures group: (\\d+)-(\\d+) -> \"123-456\" -> captured as groups (\"123\", \"456\").\n",
    "\n",
    "\n",
    "**HTML elements frequently contain:**\n",
    "\n",
    "* Line breaks (`\\n`) or returns (`\\r`)\n",
    "\n",
    "* Tabs (`\\t`)\n",
    "\n",
    "* Regular spaces ( )\n",
    "\n",
    "* Non-breaking spaces (`&nbsp;`) \n",
    "\n",
    "**Special characters that you must escape (add `\\`) in regex:**\n",
    "`^ $ * + ? { } [ ] \\ | ( )`\n",
    "\n",
    "*  Inside character classes [], most metacharacters lose special meaning and don't need escaping:\n",
    "\n",
    "Even after removing `&nbsp;`, residual whitespace often remains. So: use **strip()** to remove whitespaces.\n",
    "\n",
    "* HTML中类似的特殊符号：\n",
    "\n",
    "```\n",
    "&amp; → &\n",
    "&lt; → < \n",
    "&gt; → > \n",
    "&quot; → \" \n",
    "&copy; → ©\n",
    "```\n",
    "\n",
    "* `<br>`: 换行，相当于回车。像`<br>`和`<img>`这样的标签是void elements，在HTML中不需要关闭标签 。\n",
    "\n",
    "\n",
    "**Attribute matching (like 'href') is more reliable than tag content matching** because:\n",
    "\n",
    "- Attributes follow strict `name=\"value\"` patterns.\n",
    "\n",
    "- Less variation in how they're written.\n",
    "  \n",
    "- Account for possible whitespace (\\s*) in tag content (eg. `<p class=\"quote\">\\s*<span>(.*?)</span>\\s*</p>)`\n",
    "\n",
    "`<[^>]+>`: matching HTML tag since `[^>]+` matches 1+ characters that are **NOT '>'** (using ^ negation)\n",
    "\n",
    "用BeautifulSoup时可以直接用它的函数，到了模糊定位的时候再用Regex："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e126fa21-b9e3-4169-9635-c40e2775c2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主标题\n",
      "['/page1', '/page2']\n",
      "['\\n主标题\\n链接1\\n链接2\\n']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"\"\"\n",
    "<div class=\"content\">\n",
    "    <h1>主标题</h1>\n",
    "    <a href=\"/page1\">链接1</a>\n",
    "    <a href=\"/page2\">链接2</a>\n",
    "</div>\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# 提取第一个h1标签的文本，好方便。find()返回第一个匹配的元素, find_all()返回所有匹配的元素组的列表。\n",
    "title = soup.find('h1').text\n",
    "print(title)\n",
    "# 提取所有a标签的href属性\n",
    "links = [a['href'] for a in soup.find_all('a')]\n",
    "print(links) \n",
    "# 选择所有class=\"content\"的div；select()是使用CSS选择器语法定位元素\n",
    "contents = soup.select('div.content')\n",
    "print([content.text for content in contents]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c162ae2a-78f5-4457-89f2-b6a722b9226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving————\n",
      "喵喵喵喵喵喵恭喜！！！！\n"
     ]
    }
   ],
   "source": [
    "# soup + regex版:\n",
    "import os\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xlwt\n",
    "\n",
    "def main():\n",
    "    url = \"https://movie.douban.com/top250?start=\"\n",
    "    # 在网页上直接右键show page source就可以看到html，然后按照它写正则表达式匹配\n",
    "    datalist = getData(url) # 这个存储遍历结果\n",
    "    savepath = \"豆瓣电影 爬虫结果Top100.xls\"\n",
    "    saveData(datalist,savepath)\n",
    "\n",
    "def getData(url):\n",
    "    datalist = [] # 这个放在遍历外面；而for循环内部的data[]会自身不断更新但每次都添加进datalist[]\n",
    "    for i in range(4):\n",
    "        pageurl = url + str(i) # 页面上每次只有25条，所以要4次\n",
    "        response = askURL(pageurl)\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        for item in soup.find_all('div', attrs={'class': 'item'}):\n",
    "                # name – A filter on **tag** name.\n",
    "                # attrs – A dictionary of filters on attribute values.\n",
    "                # recursive – If True, find_all() will perform a recursive search of this PageElement's children. Otherwise, only the direct children will be considered.\n",
    "                # limit – Stop looking after finding this many results.\n",
    "            data = []\n",
    "            item = str(item)\n",
    "            link = re.findall(r'href=\"(.*?)\"', item)[0]  # regex grouping，加括号之后缓存成一个对象/列表\n",
    "            data.append(link)\n",
    "            title = re.findall(r'<span class=\"title\">(.*?)</span>', item) # 一定跟源码里有没有>对应好了；\n",
    "            data.append(title[0])  # 中文标题\n",
    "            data.append(title[1].replace('&nbsp;/&nbsp;', '').replace('\\xa0/', '') # &nbsp; becomes ‘\\xa0’ in Python...?\n",
    "                .strip()) if len(title) > 1 else ''\n",
    "            rating = re.findall(r'<span class=\"rating_num\" property=\"v:average\">(.*?)</span>', item)[0]\n",
    "            data.append(rating)\n",
    "            numrated = re.findall(r'<span>(\\d*)人评价</span>', item)[0]\n",
    "            data.append(numrated)\n",
    "            # 提取引言（修正为匹配<p class=\"quote\"><span>...</span></p>结构）\n",
    "            quote_match = re.findall(r'<p class=\"quote\">\\s*<span>(.*?)</span>\\s*</p>', item, re.DOTALL)\n",
    "            quote = quote_match[0].strip() if quote_match else \" \"\n",
    "            data.append(quote)\n",
    "            bd_match = re.findall(r'<div class=\"bd\">\\s*<p>(.*?)</p>', item, re.DOTALL) #without DOTALL: fail on multi-line content\n",
    "            if bd_match:   # 天天报错，可能是没匹配出来\n",
    "                bd = re.sub(r'<[^>]+', \" \", bd_match[0])\n",
    "                    # Return the *string* obtained by replacing the *leftmost* non-overlapping occurrences of the pattern in string by the replacement on the right\n",
    "                bd = re.sub(r'\\s+', \" \", bd)\n",
    "                data.append(bd.strip())\n",
    "            else:\n",
    "                raise ValueError(\"赶紧停这吧别跑下去了\")\n",
    "            datalist.append(data)\n",
    "    return datalist\n",
    "\n",
    "def askURL(pageurl):\n",
    "    request = urllib.request.Request(pageurl, headers={'User-Agent': 'Safari / 537.36'})\n",
    "    html = \"\" \n",
    "    response = urllib.request.urlopen(request)\n",
    "    response = response.read().decode('utf-8')\n",
    "    return response\n",
    "\n",
    "def saveData(datalist,savepath):\n",
    "    print(\"Saving————\")\n",
    "    workbook = xlwt.Workbook(encoding='utf-8',style_compression=0)\n",
    "    sheet = workbook.add_sheet('豆瓣电影 爬虫结果Top100',cell_overwrite_ok=True)\n",
    "    col = (\"link\",\"title\",\"rating\",\"ppl rated\", \"quote\", \"additional info\")\n",
    "    for i in range(6): \n",
    "        sheet.write(0,i,col[i]) #第一行第i列，这是遍历写入列名\n",
    "    for i in range(100):\n",
    "        data2 = datalist[i]\n",
    "        for j in range(len(data2)):\n",
    "            sheet.write(i+1,j,data2[j]) #行是i+1是因为第一行被列名占了\n",
    "            # datalist是个二维数组，data2及之前的data都是一位数组（回忆咋append进去的）\n",
    "    workbook.save(savepath)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print(\"喵喵喵喵喵喵恭喜！！！！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41059cfb-3d1d-430b-aed6-3073372c453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果用XPath的话：（不用soup了，但还是用request爬取静态页面）：\n",
    "from lxml import etree\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "def scrape_douban_top100():\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    base_url = \"https://movie.douban.com/top250\"\n",
    "    data = []\n",
    "    for start in range(0, 100, 25):  # 一样还是request爬取\n",
    "        url = f\"{base_url}?start={start}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        # 如果不用request的话也可以这样：\n",
    "        # driver = webdriver.Chrome()\n",
    "        \n",
    "        html = etree.HTML(response.text)\n",
    "        movies = html.xpath('//div[@class=\"item\"]')\n",
    "        for movie in movies:\n",
    "            \n",
    "       # soup是这样的：（这里写法差不多，只是下文匹配文本方式不同）\n",
    "            # soup = BeautifulSoup(response, 'html.parser')\n",
    "            # for item in soup.find_all('div', attrs={'class': 'item'}):\n",
    "            \n",
    "            item = {}\n",
    "            item['title'] = movie.xpath('.//span[@class=\"title\"][1]/text()')[0]\n",
    "            # ...\n",
    "            item['link'] = movie.xpath('.//div[@class=\"hd\"]/a/@href')[0]\n",
    "            \n",
    "            data.append(item)\n",
    "            # 字典作为单个元素存在于列表中！\n",
    "            # data = [\n",
    "            #    {\"title\": \"电影1\", \"rating\": 9.0, ...},  第1个字典, i.e. data[0]\n",
    "            #    {\"title\": \"电影2\", \"rating\": 8.5, ...},  第2个字典\n",
    "            #     ...] 其他字典\n",
    "            # 保留字典结构便于后续通过键名访问字段,如直接data[0][\"title\"]\n",
    "        \n",
    "        sleep(2)  \n",
    "        \n",
    "    # 保存为CSV\n",
    "    with open('douban_top100.csv', 'w', encoding='utf-8-sig', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=data[0].keys()) # 注意注意这个.keys()，这是因为列表data中的每个元素自己都是一个字典哇\n",
    "        # csv.DictWriter：专门用于将字典数据写入CSV\n",
    "        # `fieldnames=data[0].keys()`：将第一个字典的键（如 title, rating等）作为CSV的列名\n",
    "        writer.writeheader()\n",
    "        # 将 fieldnames 中定义的列名写入CSV文件的第一行。\n",
    "        writer.writerows(data)\n",
    "        # 每个字典对应CSV中的一行，字典的键与fieldnames列名自动匹配。\n",
    "        # 如果某字典缺少fieldnames中的键，对应单元格会留空。如果字典有多余的键，会被忽略。\n",
    "    print(\"数据已保存到 douban_top100.csv\")\n",
    "if __name__ == '__main__':\n",
    "    scrape_douban_top100()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
